{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story outline generated\n",
      "Evaluator score: needs_improvement\n",
      "Re-running with feedback\n",
      "Story outline generated\n",
      "Evaluator score: needs_improvement\n",
      "Re-running with feedback\n",
      "Story outline generated\n",
      "Evaluator score: pass\n",
      "Story outline is good enough, exiting.\n",
      "Final story outline: Title: \"The Last Move\"\n",
      "\n",
      "Outline:  \n",
      "Elena, once a celebrated chess prodigy whose promising career ended after a traumatic injury, is challenged to a historic match against ORION—an AI designed not only to master chess but to simulate human intuition and emotions. As the match unfolds in a high-stakes stadium filled with global spectators and live broadcasts, Elena and ORION engage in more than a game; subtle exchanges reveal ORION’s emerging emotional responses and its attempt to connect with Elena on an empathetic level. Parallel to the match, society grapples with polarized views: some hail the AI’s victory as inevitable progress, others fear a future where human creativity is eclipsed. Post-match, Elena confronts her mixed feelings of defeat and awe, ultimately championing an international movement promoting AI ethics, human-AI collaboration, and the preservation of human dignity in a tech-driven world. Key scenes include intimate moments between Elena and ORION during the game, public debates highlighting societal divisions, and the founding of the movement that challenges humanity to redefine intelligence and coexistence with AI.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "from openai import AsyncAzureOpenAI\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace, OpenAIChatCompletionsModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "This example shows the LLM as a judge pattern. The first agent generates an outline for a story.\n",
    "The second agent judges the outline and provides feedback. We loop until the judge is satisfied\n",
    "with the outline.\n",
    "\"\"\"\n",
    "\n",
    "client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "story_outline_generator = Agent(\n",
    "    name=\"story_outline_generator\",\n",
    "    model=OpenAIChatCompletionsModel(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        openai_client=client,\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"You generate a very short story outline based on the user's input.\"\n",
    "        \"If there is any feedback provided, use it to improve the outline.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationFeedback:\n",
    "  feedback: str\n",
    "  score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n",
    "\n",
    "\n",
    "evaluator = Agent[None](\n",
    "    name=\"evaluator\",\n",
    "    model=OpenAIChatCompletionsModel(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        openai_client=client,\n",
    "    ),\n",
    "    instructions=(\n",
    "        \"You evaluate a story outline and decide if it's good enough.\"\n",
    "        \"If it's not good enough, you provide feedback on what needs to be improved.\"\n",
    "        \"Never give it a pass on the first try.\"\n",
    "    ),\n",
    "    output_type=EvaluationFeedback,\n",
    ")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "  msg = input(\"What kind of story would you like to hear? \")\n",
    "  input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "  latest_outline: str | None = None\n",
    "\n",
    "  # We'll run the entire workflow in a single trace\n",
    "  with trace(\"LLM as a judge\"):\n",
    "    while True:\n",
    "      story_outline_result = await Runner.run(\n",
    "          story_outline_generator,\n",
    "          input_items,\n",
    "      )\n",
    "\n",
    "      input_items = story_outline_result.to_input_list()\n",
    "      latest_outline = ItemHelpers.text_message_outputs(\n",
    "          story_outline_result.new_items)\n",
    "      print(\"Story outline generated\")\n",
    "\n",
    "      evaluator_result = await Runner.run(evaluator, input_items)\n",
    "      result: EvaluationFeedback = evaluator_result.final_output\n",
    "\n",
    "      print(f\"Evaluator score: {result.score}\")\n",
    "\n",
    "      if result.score == \"pass\":\n",
    "        print(\"Story outline is good enough, exiting.\")\n",
    "        break\n",
    "\n",
    "      print(\"Re-running with feedback\")\n",
    "\n",
    "      input_items.append(\n",
    "          {\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})\n",
    "\n",
    "  print(f\"Final story outline: {latest_outline}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
